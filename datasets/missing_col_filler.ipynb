{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "model_path = \"inceptionai/jais-family-590m\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, device_map=\"cuda\", trust_remote_code=True\n",
    ")\n",
    "\n",
    "\n",
    "# Function to get the story value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses...\n",
      "Generating responses...\n",
      "Input too long for generation. Skipping...\n",
      "Generating responses...\n",
      "Input too long for generation. Skipping...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Input too long for generation. Skipping...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Input too long for generation. Skipping...\n",
      "Generating responses...\n",
      "Input too long for generation. Skipping...\n",
      "Generating responses...\n",
      "Input too long for generation. Skipping...\n",
      "Generating responses...\n",
      "Input too long for generation. Skipping...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Input too long for generation. Skipping...\n",
      "Generating responses...\n",
      "Input too long for generation. Skipping...\n",
      "Generating responses...\n",
      "Input too long for generation. Skipping...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Input too long for generation. Skipping...\n",
      "Generating responses...\n",
      "Input too long for generation. Skipping...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Input too long for generation. Skipping...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Input too long for generation. Skipping...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Input too long for generation. Skipping...\n",
      "Generating responses...\n",
      "Input too long for generation. Skipping...\n",
      "Generating responses...\n",
      "Input too long for generation. Skipping...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Input too long for generation. Skipping...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Input too long for generation. Skipping...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Input too long for generation. Skipping...\n",
      "Generating responses...\n",
      "Input too long for generation. Skipping...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Generating responses...\n",
      "Input too long for generation. Skipping...\n",
      "Generating responses...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import gc\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize the tokenizer with left padding\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"inceptionai/jais-family-590m\", padding_side=\"left\"\n",
    ")\n",
    "\n",
    "# Define the batch size for processing stories\n",
    "batch_size = 1  # Set a suitable batch size\n",
    "\n",
    "\n",
    "def batch_get_story_value(stories):\n",
    "    responses = []\n",
    "    for i in range(0, len(stories), batch_size):\n",
    "        batch = stories[i : i + batch_size]\n",
    "        prompts = [\n",
    "            (\n",
    "                f\"قم بتحليل القصة التالية واستخرج القيمة الأساسية لها. \"\n",
    "                f\"يجب أن تكون القيمة تعبر عن الدرس أو الفكرة الرئيسية من القصة. \"\n",
    "                f\"القصة هي: {story} القيمة هي: على سبيل المثال، \"\n",
    "                f\"إذا كانت القصة تتحدث عن الصداقة، يمكن أن تكون القيمة: 'الصداقة'.\"\n",
    "            )\n",
    "            for story in batch\n",
    "        ]\n",
    "\n",
    "        # Tokenize with truncation and return attention mask\n",
    "        input_ids = tokenizer(\n",
    "            prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048\n",
    "        ).input_ids\n",
    "        inputs = input_ids.to(device)\n",
    "\n",
    "        print(\"Generating responses...\")\n",
    "\n",
    "        # Set max_length for generation to prevent exceeding input size\n",
    "        max_gen_length = (\n",
    "            2048 - inputs.shape[1]\n",
    "        )  # Ensure the total length does not exceed 2048\n",
    "        if max_gen_length <= 0:\n",
    "            print(\"Input too long for generation. Skipping...\")\n",
    "            responses.extend(\n",
    "                [\"\"] * len(batch)\n",
    "            )  # Add empty responses for skipped stories\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient tracking\n",
    "            generate_ids = model.generate(\n",
    "                inputs,\n",
    "                top_p=0.95,\n",
    "                max_new_tokens=min(\n",
    "                    5, max_gen_length\n",
    "                ),  # Ensure max_new_tokens does not exceed remaining length\n",
    "                temperature=0.5,\n",
    "                repetition_penalty=1.2,\n",
    "                do_sample=True,\n",
    "            )\n",
    "\n",
    "        # Decode responses and extend the list of responses\n",
    "        responses.extend(\n",
    "            tokenizer.batch_decode(\n",
    "                generate_ids,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Clear unused variables to free up memory\n",
    "        del inputs, generate_ids, input_ids\n",
    "        gc.collect()  # Call garbage collection\n",
    "\n",
    "    return responses\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"./merged.csv\")  # Replace with your actual file path\n",
    "\n",
    "# Fill the value column\n",
    "df[\"value\"] = batch_get_story_value(df[\"content\"].tolist())\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv(\"merged_filled.csv\", index=False)  # Re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"قم بتحليل القصة التالية واستخرج القيمة الأساسية لها. يجب أن تكون القيمة تعبر عن الدرس أو الفكرة الرئيسية من القصة. القصة هي: ومما يحكى أنه في زمان الحاكم بأمر الله رجل بمصر يسمى وردان وكان جزاراً في اللحم الضأني، وكانت امرأة تأتيه كل يوم بدينار يقارب وزنه وزن دينارين ونصف من الدنانير المصرية وتقول له أعطني خروفاً وتحضر معها حمالاً بقفص فيأخذ الدينار ويعطيها خروفاً فيحمله الحمال وتأخذه وتروح به إلى مكانها، وفي ثاني يوم وقت الضحى تأتي، وكان ذلك الجزار يكتسب منها كل يوم ديناراً وأقامت مدة طويلة على ذلك. \\xa0 فتفكر وردان الجزار ذات يوم في أمرها وقال في نفسه: هذه المرأة كل يوم تشتري مني بدينار ولم تغلط يوماً واحداً وتشتري مني بدراهم هذا أمر عجيب. ثم أن وردان سأل الحمال في غيبة المرأة فقال له: أنا في غاية العجب منها فإنها كل يوم تحملني الخروف من عندك وتشتري حوائج الطعام والفاكهة والنقل بدينار آخر وتأخذ من شخص نصراني مروقتين نبيذ وتعطيه ديناراً وتحملني الجميع وأسير معها إلى بساتين الوزير، ثم تعصب عيني بحيث أني لا أنظر موضعاً من الأرض أحط فيه قدمي وتأخذ بيدي فما أعرف أين تذهب بي ثم تقول: حط هنا، وعندها قفص آخر فتعطيني الفارغ ثم تمسك يدي وتعود بي إلى الموضع الذي شدت عيني فيه بالعصابة فتحلها وتعطيني عشرة دراهم. \\xa0 فقال له الجزار: كان الله في عونها، ولكن ازداد فكراً في أمرها وكثرت عندها الوساوس وبات في قلق عظيم، ثم قال وردان الجزار: فلما أصبحت أتتني على العادة وأعطتني الدينار وأخذت الخروف وحملته للحمال وراحت فأوصيت صبي على الدكان وتبعتها بحيث لا تراني. \\xa0 وأدرك شهرزاد الصباح فسكتت عن الكلام المباح. \\xa0 وفي الليلة الثمانين بعد الثلاثمائة قالت: بلغني أيها الملك السعيد أن وردان الجزار قال: فأوصيت صبي على الدكان وتبعتها بحيث لا تراني، ولم أزل أعاينها إلى أن خرجت من مصر وأنا أتوارى خلفها حتى وصلت إلى بساتين الوزير فاختفيت حتى عصبت عيني الحمال وتبعتها من مكان إلى مكان إلى أن أتت الجبل فوصلت إلى مكان فيه حجر كبير وحطت القفص عن الحمال فصبرت إلى أن عادت بالحمال ورجعت ونزعت جميع ما كان في القفص وغابت ساعة، فأتيت إلى ذلك الحجر فزحزحته ودخلت فوجدت خلفه طابقاً من نحاس مفتوحاً ودرجاً نازلة فنزلت في تلك الدرج قليلاً قليلاً حتى وصلت إلى دهليز كثير النور فمشيت فرأيت هيئته باب قلعة فارتكنت في زوايا في زوايا الباب فوجدت صفة بها سلالم خارج باب القاعة فتعلقت فيها فوجدت صفة صغيرة بها طاقة تشرف على قاعة فنظرت في القاعة فوجدت المرأة قد أخذت الخروف وقطعت منه مطايبه وعملته في قدر ورمت الباقي قدام دب كبير عظيم الخلقة فأكله عن آخره وهي تطبخ. \\xa0 فلما فرغت أكلت كفايتها ووضعت الفاكهة والنقل وحطت النبيذ وصارت تشرب بقدح وتسقي الدب بطاسة من ذهب حتى حصل لها نشوة السكر، فنزعت ثيابها ونامت فقام الدب وواقعها وهي تعاطيه أحسن ما يكون لبني آدم حتى فرغ وجلس واستراح ولم يزل كذلك حتى فعل ذلك عشر مرات ثم وقع كل منهما مغشياً عليه وصارا لا يتحركان. \\xa0 فقلت في نفسي هذا وقت انتهاز الفرصة، فنزلت ومعي سكين تبري العظم قبل اللحم فلما صرت عندهما وجدتهما لا يتحرك فيهما عرق لما حصلت لهما من المشقة فجعلت السكين في منحر الدب واتكأت عليه حتى خلصته وانعزلت رأسه عن بدنه فصار له شخير عظيم مثل شخير الرعد، فانتبهت المرأة مرعوبة فلما رأت الدب مذبوحاً وأنا واقف والسكين في يدي زعقت زعقة عظيمة حتى ظننت أن روحها قد خرجت وقالت لي: يا وردان أيكون هذا جزاء الإحسان فقلت لها: يا عدوة نفسها هل عدمت الرجال حتى تفعلي الفعل الذميم، فأطرقت رأسها إلى الأرض لا ترد جواباً وتأملت الدب وقد نزعت رأسه عن جثته ثم قالت: يا وردان أي شيء أحب إليك أن تسمع الذي أقوله لك ويكون سبب لسلامتك. \\xa0 وأدرك شهرزاد الصباح فسكتت عن الكلام المباح. \\xa0 وفي الليلة الواحدة والثمانين بعد الثلاثمائة قالت: بلغني أيها الملك السعيد أن المرة قالت: يا وردان أي شيء أحب إليك أن تسمع الذي أقوله لك ويكون سبباً لسلامتك وغناك إلى آخر الدهر أو تخالفني ويكون سبباً لهلاكك، فقلت: أختار أن أسمع كلامك فحدثيني بما شئت، فقالت اذبحني كما ذبحت الدب وخذ من هذا الكنز حاجتك وتوجه إلى حال سبيلك، فقلت لها: أنا خير من هذا الدب فارجعي إلى الله تعالى وتوبي وأتزوج بك ونعيش باقي عمرنا بهذا الكنز قالت: أيا وردان إن هذا بعيداً كيف أعيش بعده والله إن لم تذبحني لأتلفن روحك فلا تراجعني تتلف وهذا ما عندي من الرأي والسلام فقلت: أذبحك وتروحين إلى لعنة الله ثم جذبتها من شعرها وذبحتها وراحت إلى لعنة الله والملائكة والناس جميعاً. وبعد ذلك نظرت في المحل فوجدت فيه قبة من الذهب والفصوص واللؤلؤ ما لا يقدر على جمعه أحد من الملوك فأخذت قفص الحمال وملأته على قدر ما أطيق وسترته بقماشي الذي كان علي وحملته وطلعت من الكنز وسرت ولم أزل سائراً إلى باب مصر وإذا بعشرة من جماعة الحاكم بأمر الله مقبلون والحاكم خلفهم فقال: يا وردان، قلت: لبيك أيها الملك. قال: هل قتلت الدب والمرأة؟ قلت: نعم. قال: حط عن رأسك وطب نفساً فجميع ما معك من المال لك لا ينازعك أحد عليه. فحطيت القفص بين يديه فكشفه ورآه وقال: حدثني بخبرهما وإن كنت أعرفه كأني حاضر معكم فحدثته بجميع ما جرى وهو يقول: صدقت. فقال: يا وردان قم سر بنا. فتوجهت إليه معه فوجدت الطابق مغلقاً فقال: ارفعه يا وردان فإن هذا الكنز لا يقدر أحد أن يفتحه غيرك فإنه مرصود باسمك وصفتك. فقلت: والله لا أطيق فتحه. فقال: تقدم أنت في بركة الله. فتقدمت إليه وسميت الله تعالى ومددت يدي إلى الطابق فارتفع كأنه أخف ما يمون فقال الحاكم: انزل وأطلع ما فيه فإنه لا ينزله إلا من باسمك وصورتك وصفاتك من حين وضع وقتل الدب هذا الدب وهذه المرأة على يديك وهو عندي مؤرخ وكنت أنتظر وقوعه حتى وقع. قال وردان: فنزلت ونقلت له جميع ما في الكنز ثم دعا بالدواب وحمله وأعطاني قفصي بما فيه فأخذته وعدت إلى بيتي وفتحت لي دكاناً في السوق وهذا السوق موجود إلى الآن ويعرف بسرق وردان. القيمة هي: على سبيل المثال، إذا كانت القصة تتحدث عن الصداقة، يمكن أن تكون القيمة: 'الصداقة'.\\n \\n ### ** _\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['value'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Set up your API key\n",
    "os.environ[\"GEMINI_API_KEY\"] = (\n",
    "    \"AIzaSyDV-CddABsTsA1hAfV2fZ5TMiddLNU_CyQ\"  # Replace with your actual API key\n",
    ")\n",
    "\n",
    "# Configure the Gemini API\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# Define the batch size for processing stories\n",
    "batch_size = 8  # Set a suitable batch size\n",
    "\n",
    "\n",
    "def batch_get_story_value(stories):\n",
    "    responses = []\n",
    "    for i in range(0, len(stories), batch_size):\n",
    "        batch = stories[i : i + batch_size]\n",
    "        prompts = [\n",
    "            (\n",
    "                f\"قم بتحليل القصة التالية واستخرج القيمة الأساسية لها. \"\n",
    "                f\"يجب أن تكون القيمة تعبر عن الدرس أو الفكرة الرئيسية من القصة. \"\n",
    "                f\"القصة هي: {story} القيمة هي: \"\n",
    "            )\n",
    "            for story in batch\n",
    "        ]\n",
    "\n",
    "        print(\"Generating responses using Gemini API...\")\n",
    "\n",
    "        for prompt in prompts:\n",
    "            # Create a model and run a prompt\n",
    "            model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "            response = model.generate_content(prompt)\n",
    "            responses.append(response.text)\n",
    "\n",
    "    return responses\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\n",
    "    \"./merged.csv\"\n",
    ")  # Replace with your actual file path\n",
    "\n",
    "# Fill the value column\n",
    "results = batch_get_story_value(df[\"content\"].tolist()[:10])\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "# df.to_csv(\"merged_filled.csv\", index=False)  # Replace with your desired output file path\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
